{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, how to run the anomaly detection on F16 CSAF system is described.\n",
    "\n",
    "Before anything, let's import the required libraries and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import typing as typ\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.svm import OneClassSVM\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "sns.set()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "iqn_base_path = \"./models\"\n",
    "env_name = \"CSAF_airspeed\"\n",
    "if not os.path.exists(os.path.join(iqn_base_path, env_name)):\n",
    "    os.mkdir(os.path.join(iqn_base_path, env_name))\n",
    "\n",
    "test_performances = []\n",
    "gvd_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16']\n",
    "num_inputs = 17\n",
    "num_actions = 4\n",
    "num_outputs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "First, one should generate a set of initial conditions for the system to begin with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! python ../src/generate_f16_ic.py --ic-file ic.json --n-samples 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the initial conditions for 100 samples are stored, they can be used to generate trajectory data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! python ../src/generate_dataset.py --ic-file=ic.json --config-file=../examples/f16/f16_simple_airspeed_config.toml --data-format openai --output-file data_airspeed.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have generated 100 trajectories with different length using the \"airspeed\" autopilot. These data can then be\n",
    "used to train the predictors.\n",
    "\n",
    "Predictors, or recurrent general value distributions, are very close in principle to value functions in common\n",
    "reinforcement learning literature. They estimate the expected distribution of returns given a state and horizon.\n",
    "\n",
    "Following code cell defines the architecture of the predictors (rGVDs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class RecurrentIQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, gru_size, quantile_embedding_dim, num_quantile_sample, device,\n",
    "                 fc1_units=32, fc2_units=64, fc3_units=32):\n",
    "        super(RecurrentIQN, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.gru_size = gru_size\n",
    "        self.quantile_embedding_dim = quantile_embedding_dim\n",
    "        self.num_quantile_sample = num_quantile_sample\n",
    "        self.device = device\n",
    "\n",
    "        self.gru = nn.GRUCell(num_inputs, gru_size)\n",
    "        self.post_gru = nn.Linear(gru_size, fc1_units)\n",
    "        self.fc = nn.Linear(fc1_units, num_outputs)\n",
    "\n",
    "        self.phi = nn.Linear(self.quantile_embedding_dim, 32)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, state, hx, tau, num_quantiles):\n",
    "        input_size = state.size()[0]  # batch_size(train) or 1(get_action)\n",
    "        tau = tau.expand(input_size * num_quantiles, self.quantile_embedding_dim)\n",
    "        pi_mtx = torch.Tensor(np.pi * np.arange(0, self.quantile_embedding_dim)).expand(input_size * num_quantiles,\n",
    "                                                                                        self.quantile_embedding_dim)\n",
    "        cos_tau = torch.cos(tau * pi_mtx).to(self.device)\n",
    "\n",
    "        phi = self.phi(cos_tau)\n",
    "        phi = F.relu(phi)\n",
    "\n",
    "        state_tile = state.expand(input_size, num_quantiles, self.num_inputs)\n",
    "        state_tile = state_tile.flatten().view(-1, self.num_inputs).to(self.device)\n",
    "\n",
    "        ghx = self.gru(state_tile, hx)\n",
    "        x = self.post_gru(ghx)\n",
    "        x = self.fc(x * phi)\n",
    "\n",
    "        z = x.view(-1, num_quantiles, self.num_outputs)\n",
    "\n",
    "        z = z.transpose(1, 2)  # [input_size, num_output, num_quantile]\n",
    "        return z, ghx\n",
    "\n",
    "    @classmethod\n",
    "    def train_model(cls, model, optimizer, hx, states, actions, target, batch_size, num_tau_sample, device):\n",
    "        tau = torch.Tensor(np.random.rand(batch_size * num_tau_sample, 1))\n",
    "        states = states.reshape(states.shape[0], 1, -1)\n",
    "        # states_actions = torch.cat((states, actions.unsqueeze(1)), 2)\n",
    "        # z_a, hx = model(states_actions, hx, tau, num_tau_sample)\n",
    "        z_a, hx = model(states, hx, tau, num_tau_sample)\n",
    "        z_a = torch.mean(z_a, dim=1)\n",
    "        T_z = target.to(device).unsqueeze(1).expand(-1, num_tau_sample)\n",
    "\n",
    "        error_loss = T_z - z_a\n",
    "        huber_loss = F.smooth_l1_loss(z_a, T_z.detach(), reduction='none')\n",
    "        tau = torch.arange(0, 1, 1 / num_tau_sample).view(1, num_tau_sample)\n",
    "\n",
    "        loss = (tau.to(device) - (error_loss < 0).float()).abs() * huber_loss\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss, hx\n",
    "\n",
    "    @classmethod\n",
    "    def eval_model(cls, model, hx, states, actions, target, batch_size, num_tau_sample, device):\n",
    "        tau = torch.Tensor(np.random.rand(batch_size * num_tau_sample, 1))\n",
    "        states = states.reshape(states.shape[0], 1, -1)\n",
    "        # states_actions = torch.cat((states, actions.unsqueeze(1)), 2)\n",
    "        # z_a, hx = model(states_actions, hx, tau, num_tau_sample)\n",
    "        z_a, hx = model(states, hx, tau, num_tau_sample)\n",
    "        z_a = torch.mean(z_a, dim=1)\n",
    "        T_z = target.to(device).unsqueeze(1).expand(-1, num_tau_sample)\n",
    "\n",
    "        error_loss = T_z - z_a\n",
    "        huber_loss = F.smooth_l1_loss(z_a, T_z.detach(), reduction='none')\n",
    "        tau = torch.arange(0, 1, 1 / num_tau_sample).view(1, num_tau_sample)\n",
    "\n",
    "        loss = (tau.to(device) - (error_loss < 0).float()).abs() * huber_loss\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "        return loss, hx\n",
    "\n",
    "    @classmethod\n",
    "    def test_model(cls, model, hx, states, actions, target, batch_size, num_tau_sample, device):\n",
    "        tau = torch.Tensor(np.random.rand(batch_size * num_tau_sample, 1))\n",
    "        states = states.reshape(states.shape[0], 1, -1)\n",
    "        # states_actions = torch.cat((states, actions.unsqueeze(1)), 2)\n",
    "        # z_a, hx = model(states_actions, hx, tau, num_tau_sample)\n",
    "        z_a, hx = model(states, hx, tau, num_tau_sample)\n",
    "        z_a = torch.mean(z_a, dim=1)\n",
    "        T_z = target.to(device).unsqueeze(1).expand(-1, num_tau_sample)\n",
    "\n",
    "        error_loss = T_z - z_a\n",
    "        huber_loss = F.smooth_l1_loss(z_a, T_z.detach(), reduction='none')\n",
    "        tau = torch.arange(0, 1, 1 / num_tau_sample).view(1, num_tau_sample)\n",
    "\n",
    "        loss = (tau.to(device) - (error_loss < 0).float()).abs() * huber_loss\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "        return z_a.squeeze(0), loss, hx\n",
    "\n",
    "    @classmethod\n",
    "    def feed_forward(cls, model, hx, states, batch_size, num_tau_sample):\n",
    "        tau = torch.Tensor(np.random.rand(batch_size * num_tau_sample, 1))\n",
    "        states = states.reshape(states.shape[0], 1, -1)\n",
    "        # states_actions = torch.cat((states, actions.unsqueeze(1)), 2)\n",
    "        # z_a, hx = model(states_actions, hx, tau, num_tau_sample)\n",
    "        z_a, hx = model(states, hx, tau, num_tau_sample)\n",
    "        z_a = torch.mean(z_a, dim=1)\n",
    "        return z_a.squeeze(0), hx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to make the data ready for training, the following function can be used. It basically calculates the return\n",
    "of each predictor based on the current state and horizon, and then shuffles them for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def construct_gvd_data_undiscounted(input_len, dataset, batch_size, horizon, device, gvd_name, sum_type):\n",
    "    states, actions = [], []\n",
    "    episodes_states = []\n",
    "    episodes_actions = []\n",
    "    episodes_returns = []\n",
    "    episodes_len = []\n",
    "    all_states = []\n",
    "    assert len(dataset) != 0, \"Memory is empty!\"\n",
    "    for i, data in enumerate(dataset):\n",
    "        for j in range(len(data['time'])):\n",
    "            all_states.append(np.array(data['state'][j]))\n",
    "\n",
    "    for i, data in enumerate(dataset):\n",
    "        for j in range(len(data['time'])):\n",
    "            states.append(np.array(data['state'][j]))\n",
    "            actions.append(np.array(data['actions'][j]))\n",
    "\n",
    "        normalized_states = (np.array(states) - np.array(all_states).min(axis=0)) / (np.array(all_states).max(axis=0) - np.array(all_states).min(axis=0))\n",
    "        returns = np.zeros(normalized_states.shape[0])\n",
    "        for j in range(normalized_states.shape[0]):\n",
    "            feature_index = int(gvd_name)\n",
    "            if sum_type == \"delta\":\n",
    "                returns[j] = sum(np.diff(normalized_states[j: j + horizon + 1, feature_index]))\n",
    "            elif sum_type == \"abs_delta\":\n",
    "                returns[j] = sum(abs(np.diff(normalized_states[j: j + horizon + 1, feature_index])))\n",
    "            elif sum_type == \"time_avg\":\n",
    "                returns[j] = sum(np.diff(normalized_states[j: j + horizon + 1, feature_index])) / len(normalized_states[j: j + horizon, feature_index])\n",
    "            else:\n",
    "                assert False, \"Undefined/unknown method given to calculate the target return for GVDs. Notice the\" \\\n",
    "                              \" given arguments!\"\n",
    "\n",
    "        episodes_states.append(states)\n",
    "        episodes_actions.append(actions)\n",
    "        episodes_returns.append(returns)\n",
    "        episodes_len.append(len(states))\n",
    "        states = []\n",
    "        actions = []\n",
    "\n",
    "    max_len = len(max(episodes_states, key=len))\n",
    "    for i, _ in enumerate(episodes_states):\n",
    "        episodes_states[i] = np.concatenate((episodes_states[i], np.zeros((max_len - len(episodes_states[i]), input_len))), axis=0)\n",
    "        episodes_actions[i] = np.concatenate((episodes_actions[i], np.zeros((max_len - len(episodes_actions[i]), num_actions))), axis=0)\n",
    "        episodes_returns[i] = np.concatenate((episodes_returns[i], np.zeros((max_len - len(episodes_returns[i])))), axis=0)\n",
    "\n",
    "        episodes_states[i] = torch.Tensor(episodes_states[i]).to(device)\n",
    "        episodes_actions[i] = torch.Tensor(episodes_actions[i]).to(device)\n",
    "        episodes_returns[i] = torch.Tensor(episodes_returns[i]).to(device)\n",
    "\n",
    "    episodes_states = torch.stack(episodes_states)\n",
    "    episodes_actions = torch.stack(episodes_actions)\n",
    "    episodes_returns = torch.stack(episodes_returns)\n",
    "    episodes_len = torch.Tensor(episodes_len).to(device)[:, None, None]\n",
    "\n",
    "    tensor_dataset = torch.utils.data.TensorDataset(episodes_states, episodes_actions, episodes_returns, episodes_len)\n",
    "    all_indices = np.arange(len(episodes_states))\n",
    "    np.random.shuffle(all_indices)\n",
    "    train_indices = all_indices[:int(len(all_indices) * 90 / 100)]\n",
    "    test_indices = all_indices[int(len(all_indices) * 90 / 100):]\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "    train_dl = DataLoader(tensor_dataset, batch_size, sampler=train_sampler)\n",
    "    test_dl = DataLoader(tensor_dataset, batch_size, sampler=test_sampler)\n",
    "    return train_dl, test_dl, max_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These following functions form the training step, validation step, switching between training and validation every given\n",
    " interval, to see how rGVD is performing. At each iteration, a plot of training loss and validation loss is saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def learn_undiscounted(model, optimizer, memory, max_len, gru_size, num_tau_sample, device):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "\n",
    "    for s_batch, a_batch, mc_returns, _ in memory:\n",
    "        h_gvfs = None\n",
    "        for i in range(max_len):\n",
    "            s, a, mc_return = s_batch[:, i, :], a_batch[:, i, :], mc_returns[:, i]\n",
    "            if h_gvfs is None:\n",
    "                h_gvfs = torch.zeros(len(s_batch) * num_tau_sample, gru_size)\n",
    "            loss, h_gvfs = RecurrentIQN.train_model(model, optimizer, h_gvfs.detach().to(device), s, a, mc_return, len(s_batch), num_tau_sample, device)\n",
    "            total_loss += loss\n",
    "            count += 1\n",
    "\n",
    "    return total_loss / count\n",
    "\n",
    "def save_model(model, file_path):\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "\n",
    "def evaluation_undiscounted(model, memory, max_len, gru_size, num_tau_sample, device, best_gvf_total_loss, is_test=False):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    for s_batch, a_batch, mc_returns, _ in memory:\n",
    "        h_gvfs = None\n",
    "        for i in range(max_len):\n",
    "            s, a, mc_return = s_batch[:, i, :], a_batch[:, i, :], mc_returns[:, i]\n",
    "            if h_gvfs is None:\n",
    "                h_gvfs = torch.zeros(len(s_batch) * num_tau_sample, gru_size)\n",
    "\n",
    "            loss, h_gvfs = RecurrentIQN.eval_model(model, h_gvfs.detach().to(device), s, a, mc_return, len(s_batch), num_tau_sample, device)\n",
    "            total_loss += loss\n",
    "            count += 1\n",
    "    if not is_test:\n",
    "        print(\"GVD avg loss is:\", round(total_loss.item() / count, 3))\n",
    "        if total_loss.item() / count <= best_gvf_total_loss:\n",
    "            print(\"Saving the best model!\")\n",
    "            best_gvf_total_loss = total_loss.item() / count\n",
    "#             save_model(model, \"./models/\" + env_name + \"/\" + gvd_name + \"_gvd_\" + target_return_type + \"_h_\" + str(horizon[0]) + \".pt\")\n",
    "    return round(total_loss.item() / count, 3), best_gvf_total_loss\n",
    "\n",
    "\n",
    "def plot_losses(train_loss, test_loss, result_folder, horizon, gvd_name, info, bootstrapped):\n",
    "    plt.plot(train_loss, label=\"training loss\")\n",
    "    plt.plot(test_loss, label=\"test loss\")\n",
    "    plt.legend()\n",
    "    if not bootstrapped:\n",
    "        plt.savefig(os.path.join(result_folder, \"losses_\" + gvd_name + \"_\" + info + \"_h\" + str(horizon) + \".png\"))\n",
    "    else:\n",
    "        plt.savefig(os.path.join(result_folder, \"losses_\" + gvd_name + \"_\" + info + \"_h\" + str(horizon) + \"_bootstrap.png\"))\n",
    "    plt.clf()\n",
    "\n",
    "def update_recurrent_gvds(train_memory, test_memory, r_gvd_model, optimizer, device, horizon, max_len):\n",
    "    all_train_losses, all_test_losses = [], []\n",
    "    best_gvf_total_loss = float(\"inf\")\n",
    "    for i in range(num_iterations):\n",
    "        total_loss = learn_undiscounted(r_gvd_model, optimizer, train_memory, max_len, gru_units, num_tau_sample, device)\n",
    "        if i % test_interval == 0:\n",
    "            print(\"train loss : {}\".format(total_loss))\n",
    "            all_train_losses.append(total_loss)\n",
    "            avg_eval_loss, best_gvf_total_loss = evaluation_undiscounted(r_gvd_model, test_memory, max_len, gru_units, num_tau_sample, device, best_gvf_total_loss)\n",
    "            all_test_losses.append(avg_eval_loss)\n",
    "            plot_losses(all_train_losses, all_test_losses, os.path.join(iqn_base_path, env_name), horizon,\n",
    "                        gvd_name, target_return_type, bootstrapped=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that all the necessary functions for training predictors have been defined, we can go ahead and start the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GVD training data!\n",
      "GVD data loaded!\n",
      "Loading pre-trained model!\n",
      "Pre-trained model loaded!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-dad49df258ba>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0mrecurrent_gvd_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m train_rb, test_rb, max_len = construct_gvd_data_undiscounted(num_inputs, memory, batch_size, horizon[0],\n\u001B[0;32m---> 31\u001B[0;31m                                                              device, gvd_name, target_return_type)\n\u001B[0m\u001B[1;32m     32\u001B[0m \u001B[0mupdate_recurrent_gvds\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_rb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_rb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrecurrent_gvd_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhorizon\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_len\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-8-a690fcc0c5da>\u001B[0m in \u001B[0;36mconstruct_gvd_data_undiscounted\u001B[0;34m(input_len, dataset, batch_size, horizon, device, gvd_name, sum_type)\u001B[0m\n\u001B[1;32m     25\u001B[0m                 \u001B[0mreturns\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mabs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdiff\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnormalized_states\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mj\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mhorizon\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeature_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0msum_type\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"time_avg\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 27\u001B[0;31m                 \u001B[0mreturns\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdiff\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnormalized_states\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mj\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mhorizon\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeature_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnormalized_states\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mj\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mhorizon\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeature_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     28\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m                 \u001B[0;32massert\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"Undefined/unknown method given to calculate the target return for GVDs. Notice the\"\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "data_path = \"data_airspeed.json\"\n",
    "test_data_path = \"data_airspeed_test.json\"\n",
    "noisy_data_path = \"data_airspeed_noisy.json\"\n",
    "gru_units = 32\n",
    "quantile_embedding_dim = 64\n",
    "num_quantile_sample = 32\n",
    "gvd_name = \"0\"\n",
    "target_return_type = \"time_avg\"\n",
    "horizon = [1]\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "num_iterations = 100000\n",
    "test_interval = 10\n",
    "num_tau_sample = 16\n",
    "score_calc_method = \"knn\"\n",
    "merge_type = \"avg\"\n",
    "\n",
    "print(\"Loading GVD training data!\")\n",
    "with open(data_path) as f:\n",
    "    memory = json.load(f)\n",
    "print(\"GVD data loaded!\")\n",
    "\n",
    "recurrent_gvd_model = RecurrentIQN(num_inputs, num_outputs, gru_units, quantile_embedding_dim,\n",
    "                                   num_quantile_sample, device)\n",
    "gvd_path = os.path.join(iqn_base_path, env_name, gvd_name + \"_gvd_\" + target_return_type + \"_h_\"\n",
    "                        + str(horizon[0]) + \".pt\")\n",
    "if os.path.exists(gvd_path):\n",
    "    print(\"Loading pre-trained model!\")\n",
    "    recurrent_gvd_model.load_state_dict(torch.load(gvd_path, map_location=device))\n",
    "    print(\"Pre-trained model loaded!\")\n",
    "optimizer = optim.Adam(recurrent_gvd_model.parameters(), lr=lr)\n",
    "recurrent_gvd_model.to(device)\n",
    "recurrent_gvd_model.train()\n",
    "train_rb, test_rb, max_len = construct_gvd_data_undiscounted(num_inputs, memory, batch_size, horizon[0],\n",
    "                                                             device, gvd_name, target_return_type)\n",
    "update_recurrent_gvds(train_rb, test_rb, recurrent_gvd_model, optimizer, device, horizon[0], max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "When predictors are trained, we can simply run them over a nominal *unseen* trajectory to see how they perform in terms \n",
    "of predicting the features throughout the trajectory. First, we need to prepare data for test, define the test function,\n",
    "and define the function which plot the test accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def construct_test_gvd_data_undiscounted(input_len, dataset, train_dataset, batch_size, horizon, device, sum_type):\n",
    "    states, actions = [], []\n",
    "    episodes_states = []\n",
    "    episodes_actions = []\n",
    "    episodes_returns = []\n",
    "    episodes_len = []\n",
    "    all_training_states = []\n",
    "    assert len(dataset) != 0, \"Memory is empty!\"\n",
    "\n",
    "    for i, data in enumerate(train_dataset):\n",
    "        for j in range(len(data['time'])):\n",
    "            all_training_states.append(np.array(data['state'][j]))\n",
    "\n",
    "    for j in range(len(dataset['time'])):\n",
    "        states.append(np.array(dataset['state'][j]))\n",
    "        actions.append(np.array(dataset['actions'][j]))\n",
    "\n",
    "    normalized_states = (np.array(states) - np.array(all_training_states).min(axis=0)) / (np.array(all_training_states).max(axis=0) - np.array(all_training_states).min(axis=0))\n",
    "    returns = []\n",
    "    for j in range(normalized_states.shape[0]):\n",
    "        if sum_type == \"delta\":\n",
    "            returns.append(sum(np.diff(normalized_states[j: j + horizon + 1], axis=0)))\n",
    "        elif sum_type == \"abs_delta\":\n",
    "            returns.append(sum(abs(np.diff(normalized_states[j: j + horizon + 1], axis=0))))\n",
    "        elif sum_type == \"time_avg\":\n",
    "            if j == normalized_states.shape[0] - 1:\n",
    "                returns.append(np.zeros(input_len))\n",
    "            else:\n",
    "                returns.append(sum(np.diff(normalized_states[j: j + horizon + 1], axis=0)) / len(normalized_states[j: j + horizon]))\n",
    "        else:\n",
    "            assert False, \"Undefined/unknown method given to calculate the target return for GVDs. Notice the\" \\\n",
    "                          \" given arguments!\"\n",
    "\n",
    "    episodes_states.append(states)\n",
    "    episodes_actions.append(actions)\n",
    "    episodes_returns.append(np.array(returns))\n",
    "    episodes_len.append(len(states))\n",
    "\n",
    "    max_len = len(max(episodes_states, key=len))\n",
    "    episodes_states[0] = torch.Tensor(episodes_states[0]).to(device)\n",
    "    episodes_actions[0] = torch.Tensor(episodes_actions[0]).to(device)\n",
    "    episodes_returns[0] = torch.Tensor(episodes_returns[0]).to(device)\n",
    "\n",
    "    episodes_states = torch.stack(episodes_states)\n",
    "    episodes_actions = torch.stack(episodes_actions)\n",
    "    episodes_returns = torch.stack(episodes_returns)\n",
    "    episodes_len = torch.Tensor(episodes_len).to(device)[:, None, None]\n",
    "\n",
    "    tensor_dataset = torch.utils.data.TensorDataset(episodes_states, episodes_actions, episodes_returns, episodes_len)\n",
    "    all_indices = np.arange(len(episodes_states))\n",
    "    test_sampler = SubsetRandomSampler(all_indices)\n",
    "    test_dl = DataLoader(tensor_dataset, batch_size, sampler=test_sampler)\n",
    "    return test_dl, max_len\n",
    "\n",
    "def test_undiscounted(model, memory, max_len, gru_size, num_tau_sample, device, gvd_name):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    model.eval()\n",
    "    dists = []\n",
    "    mcs = []\n",
    "    for s_batch, a_batch, mc_returns, _ in memory:\n",
    "        h_gvfs = None\n",
    "        for i in range(max_len):\n",
    "            s, a, mc_return = s_batch[:, i, :], a_batch[:, i, :], mc_returns[:, i, gvd_name]\n",
    "            if h_gvfs is None:\n",
    "                h_gvfs = torch.zeros(len(s) * num_tau_sample, gru_size)\n",
    "\n",
    "            distributional_return, loss, h_gvfs = RecurrentIQN.test_model(model, h_gvfs.detach().to(device), s, a, mc_return, len(s), num_tau_sample, device)\n",
    "            dists.append(distributional_return.squeeze(0).detach().cpu().numpy())\n",
    "            mcs.append(mc_return.item())\n",
    "            total_loss += loss\n",
    "            count += 1\n",
    "    return mcs, dists\n",
    "\n",
    "def plot_rgvd_accuracy(results, result_folder, horizon, info, plot_dist=True):\n",
    "    fig, axs = plt.subplots(math.ceil(len(results) / 3), 3, figsize=(20, 20))\n",
    "    r, c = 0, 0\n",
    "    for key in results.keys():\n",
    "        if plot_dist:\n",
    "            axs[r, c].plot(results[key][1], color='limegreen')\n",
    "            # for i in range(len(results[key][1])):\n",
    "            #     scattered_dist = np.zeros((len(results[key][1][i]))) + i\n",
    "            #     axs[r, c].scatter(scattered_dist, results[key][1][i], color='limegreen', s=10)\n",
    "        else:\n",
    "            axs[r, c].plot(np.array(results[key][1]).mean(axis=1), color='limegreen')\n",
    "            axs[r, c].plot(np.array(results[key][1]).max(axis=1), color='palegreen')\n",
    "            axs[r, c].plot(np.array(results[key][1]).min(axis=1), color='palegreen')\n",
    "        axs[r, c].plot(results[key][0], color='teal')\n",
    "        axs[r, c].set(xlabel='step', ylabel='return')\n",
    "        axs[r, c].set_title(\"GVD: \" + key.split(\"_\")[0])\n",
    "        if r < math.ceil(len(results) / 3) - 1:\n",
    "            r += 1\n",
    "        else:\n",
    "            c += 1\n",
    "            r = 0\n",
    "    if plot_dist:\n",
    "        labels = [\"actual MC returns\", \"rGVD returns\"]\n",
    "        fig.legend(labels=labels, labelcolor=['teal', 'limegreen'], handlelength=0)\n",
    "    else:\n",
    "        labels = [\"actual MC returns\", \"rGVD returns mean\", \"rGVD returns min & max\"]\n",
    "        fig.legend(labels=labels, labelcolor=['teal', 'limegreen', 'palegreen'], handlelength=0)\n",
    "    # fig.show()\n",
    "    fig.suptitle(\"Recurrent GVD accuracy\\nhorizon: \" + str(horizon) + \"\\n\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(result_folder, \"rGVD_accuracy_\" + info + \"_h\" + str(horizon) + \".png\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once the defining step is completed, we shall start the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Loading GVD training data!\")\n",
    "with open(data_path) as f:\n",
    "    train_memory = json.load(f)\n",
    "print(\"GVD training data loaded!\")\n",
    "test_results = {}\n",
    "print(\"Loading GVD test data!\")\n",
    "with open(test_data_path) as f:\n",
    "    memory = json.load(f)\n",
    "memory = memory[random.randint(0, len(memory) - 1)]\n",
    "# memory = memory[45]\n",
    "# memory = memory[33]\n",
    "print(\"GVD test data loaded!\")\n",
    "test_rb, max_len = construct_test_gvd_data_undiscounted(num_inputs, memory, train_memory, batch_size,\n",
    "                                                        horizon[0], device, target_return_type)\n",
    "for rgvd_name in gvd_names:\n",
    "    recurrent_gvd_model = RecurrentIQN(num_inputs, num_outputs, gru_units, quantile_embedding_dim,\n",
    "                                       num_quantile_sample, device)\n",
    "    gvd_path = os.path.join(iqn_base_path, env_name,\n",
    "                            rgvd_name + \"_gvd_\" + target_return_type + \"_h_\" + str(horizon[0]) + \".pt\")\n",
    "    recurrent_gvd_model.load_state_dict(torch.load(gvd_path, map_location=device))\n",
    "    recurrent_gvd_model.to(device)\n",
    "    recurrent_gvd_model.eval()\n",
    "\n",
    "    actual_returns, dist_returns = test_undiscounted(recurrent_gvd_model, test_rb, max_len, gru_units,\n",
    "                                                     num_tau_sample, device, int(rgvd_name))\n",
    "\n",
    "    test_results[rgvd_name] = (actual_returns, dist_returns)\n",
    "plot_rgvd_accuracy(test_results, os.path.join(iqn_base_path, env_name), horizon[0],\n",
    "                   target_return_type, plot_dist=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the last step, using the predictors, we want to detect anomalies happening in an anomalous environment. But before that,\n",
    "we need to have trajectories representing the anomalous systems. This can be done simply by just going to the first step\n",
    " and instead of using the nominal system configuration file, use the noisy system configuration. Once the noisy data are\n",
    "generated and stored, anomaly detection can be started."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def local_outlier_factor(distribution, actual_return):\n",
    "    lof = LocalOutlierFactor(n_neighbors=8)\n",
    "    lof.fit_predict(np.append(distribution, actual_return).reshape(-1, 1))\n",
    "    score = abs(lof.negative_outlier_factor_[-1])\n",
    "    return score\n",
    "\n",
    "\n",
    "def k_nearest_neighbors(distribution, actual_return):\n",
    "    neigh = NearestNeighbors(n_neighbors=8)\n",
    "    neigh.fit(distribution.reshape(-1, 1))\n",
    "    distances, indices = neigh.kneighbors(np.array(actual_return).reshape(-1, 1))\n",
    "    return distances.sum()\n",
    "\n",
    "\n",
    "def isolation_forest(distribution, actual_return):\n",
    "    clf = IsolationForest(n_estimators=10, contamination=0.03)\n",
    "    clf.fit(distribution.reshape(-1, 1))\n",
    "    score = abs(clf.score_samples(np.array(actual_return).reshape(-1, 1)))[0]\n",
    "    return score\n",
    "    # return 0\n",
    "\n",
    "\n",
    "def measure_as(as_method, value_dist, ac_return):\n",
    "    if as_method == \"lof\":\n",
    "        score = local_outlier_factor(value_dist, round(ac_return, 5))\n",
    "    elif as_method == \"knn\":\n",
    "        score = k_nearest_neighbors(value_dist, round(ac_return, 5))\n",
    "    elif as_method == \"iforest\":\n",
    "        score = isolation_forest(value_dist, round(ac_return, 5))\n",
    "    else:\n",
    "        assert False, \"Anomaly score measuring method is not given properly! Check '--score_calc_method'!\"\n",
    "    return score\n",
    "\n",
    "\n",
    "def anomaly_detection(all_models, memory, max_len, gru_size, num_tau_sample, device, as_method, h, merge_type):\n",
    "    ep_scores = {}\n",
    "    ep_dists = {}\n",
    "    mcs = {}\n",
    "    hs_dict = {}\n",
    "    for _, rgvd_name in all_models:\n",
    "        ep_scores[rgvd_name] = []\n",
    "        ep_dists[rgvd_name] = []\n",
    "        mcs[rgvd_name] = []\n",
    "        hs_dict[rgvd_name] = torch.zeros(num_tau_sample, gru_size)\n",
    "\n",
    "    for s_batch, a_batch, mc_returns, _ in memory:\n",
    "        for i in range(max_len):\n",
    "            s, a, mc_return = s_batch[:, i, :], a_batch[:, i, :], mc_returns[:, i]\n",
    "\n",
    "            for rgvd_model, rgvd_name in all_models:\n",
    "                distributional_return, h_gvfs = RecurrentIQN.feed_forward(rgvd_model, hs_dict[rgvd_name].detach().to(device),\n",
    "                                                                          s, len(s), num_tau_sample)\n",
    "                hs_dict[rgvd_name] = h_gvfs\n",
    "                anomaly_score = measure_as(as_method, distributional_return.squeeze(0).detach().cpu().numpy(),\n",
    "                                           mc_return.squeeze(0)[int(rgvd_name)].item())\n",
    "                ep_scores[rgvd_name].append(anomaly_score)\n",
    "                ep_dists[rgvd_name].append(distributional_return.squeeze(0).detach().cpu().numpy())\n",
    "                mcs[rgvd_name].append(mc_return.squeeze(0)[int(rgvd_name)].item())\n",
    "\n",
    "    if merge_type == \"avg\":\n",
    "        scores_merged = np.zeros(max_len)\n",
    "        for key, values in ep_scores.items():\n",
    "            scores_merged += np.array(values).copy()\n",
    "    elif merge_type == \"max\":\n",
    "        scores_merged = []\n",
    "        for key, values in ep_scores.items():\n",
    "            scores_merged.append(values)\n",
    "        scores_merged = np.array(scores_merged).max(axis=0)\n",
    "    return ep_scores, scores_merged, mcs, ep_dists\n",
    "\n",
    "def merged_confusion_matrix(nominal_scores, anom_scores):\n",
    "    scores = np.append(nominal_scores, anom_scores)\n",
    "\n",
    "    norm_labels = np.zeros(len(nominal_scores))\n",
    "    anorm_labels = np.ones(len(anom_scores))\n",
    "\n",
    "    labels = np.append(norm_labels, anorm_labels)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    auc = sklearn.metrics.auc(fpr, tpr)\n",
    "    results = (fpr, tpr, thresholds, auc)\n",
    "    return results\n",
    "\n",
    "def separated_confusion_matrix(nominal_scores, anom_scores):\n",
    "    results = {}\n",
    "    for key in nominal_scores.keys():\n",
    "        scores = np.append(nominal_scores[key], anom_scores[key])\n",
    "\n",
    "        norm_labels = np.zeros(len(nominal_scores[key]))\n",
    "        anorm_labels = np.ones(len(anom_scores[key]))\n",
    "\n",
    "        labels = np.append(norm_labels, anorm_labels)\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        results[key] = (fpr, tpr, thresholds, auc)\n",
    "    return results\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "print(\"Loading noisy data!\")\n",
    "with open(noisy_data_path) as f:\n",
    "    anomalous_memory = json.load(f)\n",
    "print(\"Noisy data loaded!\")\n",
    "print(\"Loading GVD training data!\")\n",
    "with open(data_path) as f:\n",
    "    train_memory = json.load(f)\n",
    "print(\"GVD training data loaded!\")\n",
    "print(\"Loading nominal data!\")\n",
    "with open(test_data_path) as f:\n",
    "    nominal_memory = json.load(f)\n",
    "print(\"Nominal data loaded!\")\n",
    "all_data_merged_results = []\n",
    "all_data_separated_results = []\n",
    "for data_index in range(len(anomalous_memory)):\n",
    "    single_anomalous_memory = anomalous_memory[data_index]\n",
    "    single_nominal_memory = nominal_memory[data_index]\n",
    "    merged_results = {}\n",
    "    separated_results = {}\n",
    "    for h in horizon:\n",
    "        all_rgvd_models = []\n",
    "        for rgvd_name in gvd_names:\n",
    "            recurrent_gvd_model = RecurrentIQN(num_inputs, num_outputs, gru_units, quantile_embedding_dim,\n",
    "                                               num_quantile_sample, device)\n",
    "            gvd_path = os.path.join(iqn_base_path, env_name,\n",
    "                                    rgvd_name + \"_gvd_\" + target_return_type + \"_h_\" + str(h) + \".pt\")\n",
    "            recurrent_gvd_model.load_state_dict(torch.load(gvd_path, map_location=device))\n",
    "            recurrent_gvd_model.to(device)\n",
    "            recurrent_gvd_model.eval()\n",
    "            all_rgvd_models.append((recurrent_gvd_model, rgvd_name))\n",
    "\n",
    "        anomalous_rb, anom_max_len = construct_test_gvd_data_undiscounted(num_inputs, single_anomalous_memory, train_memory,\n",
    "                                                                          batch_size, h, device, target_return_type)\n",
    "\n",
    "        rgvds_anomaly_scores_a, merged_anomaly_scores_a, mcs_anom, dists_anom = anomaly_detection(all_rgvd_models, anomalous_rb, anom_max_len,\n",
    "                                                                            gru_units, num_tau_sample, device,\n",
    "                                                                            score_calc_method, h, merge_type)\n",
    "\n",
    "        nominal_rb, nom_max_len = construct_test_gvd_data_undiscounted(num_inputs, single_nominal_memory, train_memory,\n",
    "                                                                       batch_size, h, device, target_return_type)\n",
    "\n",
    "        rgvds_anomaly_scores_n, merged_anomaly_scores_n, mcs_nom, dists_nom = anomaly_detection(all_rgvd_models, nominal_rb, nom_max_len,\n",
    "                                                                            gru_units, num_tau_sample, device,\n",
    "                                                                            score_calc_method, h, merge_type)\n",
    "        merged_results[h] = merged_confusion_matrix(merged_anomaly_scores_n, merged_anomaly_scores_a)\n",
    "        separated_results[h] = separated_confusion_matrix(rgvds_anomaly_scores_n, rgvds_anomaly_scores_a)\n",
    "        data_comparison = {}\n",
    "        for rgvd_name in gvd_names:\n",
    "            anom_mcs = []\n",
    "            nom_mcs = []\n",
    "            for _, _, mc_returns, _ in anomalous_rb:\n",
    "                for i in range(min(nom_max_len, anom_max_len)):\n",
    "                    mc_return = mc_returns[:, i, int(rgvd_name)]\n",
    "                    anom_mcs.append(mc_return.item())\n",
    "            for _, _, mc_returns, _ in nominal_rb:\n",
    "                for i in range(min(nom_max_len, anom_max_len)):\n",
    "                    mc_return = mc_returns[:, i, int(rgvd_name)]\n",
    "                    nom_mcs.append(mc_return.item())\n",
    "            data_comparison[rgvd_name] = (nom_mcs, anom_mcs)\n",
    "        print(\"Processing for h =\", str(h), \"is done! Move on to the next step!\")\n",
    "    all_data_merged_results.append(merged_results)\n",
    "    all_data_separated_results.append(separated_results)\n",
    "\n",
    "each_horizon_merged_auc = {}\n",
    "each_horizon_separated_auc = {}\n",
    "for h in horizon:\n",
    "    each_horizon_merged_auc[h] = []\n",
    "    for item in all_data_merged_results:\n",
    "        each_horizon_merged_auc[h].append(item[h][3])\n",
    "    each_horizon_separated_auc[h] = []\n",
    "    for gvd_n in gvd_names:\n",
    "        tmp_storage = []\n",
    "        for item in all_data_separated_results:\n",
    "            tmp_storage.append(item[h][gvd_n][3])\n",
    "        each_horizon_separated_auc[h].append(tmp_storage)\n",
    "\n",
    "print(\"Number of runs:\", len(anomalous_memory))\n",
    "for h in horizon:\n",
    "    print(\"-------- horizon\", h, \"--------\")\n",
    "    print(\"Max combined AUC:\", round(np.array(each_horizon_merged_auc[h]).max(), 3))\n",
    "    print(\"Min combined AUC:\", round(np.array(each_horizon_merged_auc[h]).min(), 3))\n",
    "    print(\"Average combined AUC:\", round(np.array(each_horizon_merged_auc[h]).mean(), 3))\n",
    "    print(\"Individual feature with max AUC (feature #, AUC):\", (np.array(each_horizon_separated_auc[h]).mean(axis=1).argmax(),\n",
    "                                                                round(np.array(each_horizon_separated_auc[h]).mean(axis=1).max(), 3)))\n",
    "    print(\"Individual feature with min AUC (feature #, AUC):\", (np.array(each_horizon_separated_auc[h]).mean(axis=1).argmin(),\n",
    "                                                                round(np.array(each_horizon_separated_auc[h]).mean(axis=1).min(), 3)))\n",
    "    print(\"Individual features' AUCs (feature #, AUC):\", end=' ')\n",
    "    for k in range(len(each_horizon_separated_auc[h])):\n",
    "        print((k, round(np.array(each_horizon_separated_auc[h][k]).mean(), 3)), end=' ')\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}